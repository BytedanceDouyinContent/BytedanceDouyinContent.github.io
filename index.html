<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SAIL-VL: Scalable Vision Language Model Training via High Quality Data Curation">
  <meta property="og:title" content="SAIL-VL: Scalable Vision Language Model Training via High Quality Data Curation" />
  <meta property="og:description"
    content="SAIL-VL: Scalable Vision Language Model Training via High Quality Data Curation" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/sail.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="SAIL-VL: Scalable Vision Language Model Training via High Quality Data Curation">
  <meta name="twitter:description"
    content="SAIL-VL: Scalable Vision Language Model Training via High Quality Data Curation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/sail.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="SAIL-VL,VLM,LLM,Multimodel,Data">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SAIL-VL: Scalable Vision Language Model Training via High Quality Data Curation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SAIL-VL: Scalable Vision Language Model Training via High Quality
              Data Curation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Hongyuan Dong<sup>*</sup>,</span>
              <span class="author-block">Zijian Kang<sup>*</sup>,</span>
              <span class="author-block">Weijie Yin<sup>*</sup>,</span>
              <span class="author-block">Xiao Liang,</span>
              <span class="author-block">Chao Feng,</span>
              <span class="author-block">Jiao Ran,</span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Bytedance Douyin Content<br>2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2501.05952v2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>



                <!-- Github link -->
                <span class="link-block">
                  <a href="https://huggingface.co/BytedanceDouyinContent/SAIL-VL-2B" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <p style="font-size:18px">ðŸ¤—</p>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.05952v2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>In the existing research related to VLMs, there are two problems that lead to poor performance of
              lightweight VLMs:</p>
            <ul>
              <li>The quality of the basic visual understanding-type data used in VLM pre-training and the total
                amount of training are insufficient. This results in relatively weak basic visual understanding
                capabilities of the model during the pre-training stage, thus affecting the overall performance of the
                model after SFT.</li>
              <li>There is a lack of a unified methodology for the ratio scheme of visual instruction fine-tuning data
                in the VLM SFT stage and the arrangement of the training stage. Training SFT data of different scales
                and complexity levels together will affect the overall effect of the model.</li>
            </ul>
            <p>To address these issues, we proposed a scalable high-quality detail caption data production
              solution and expanded the scale of pre-training data for SAIL-VL-2B to 655B tokens, observing stable
              data size scaling laws. In the SFT stage, we address the importance of
              high-quality SFT data and the design of the training stage, we observed a trend of the model
              performance improving with the increase in the amount of training data.</p>

            <img src="static/images/paper_page.png" alt="pipline" width="100%">

            <!-- <p>
            We introduce SAIL-VL, an open-source vision language model (VLM) series achieving state-of-the-art (SOTA) performance in 2B and 8B parameters. The following three key improvements contribute to SAIL-VL's leading performance: (1) Scalable high-quality visual understanding data construction: We implement a data construction pipeline to enable hundred-million-scale high-quality recaption data annotation, and the resulted dataset SAIL-Caption is validated to be of the highest data quality compared with opensource alternatives. (2) Scalable Pretraining with High-Quality Visual Understanding Data: We scale SAIL-VL's pretraining budget up to 655B tokens and show that even a 2B VLM benefits from scaled up training data sizes, exhibiting expected data size scaling laws in visual understanding and instruction following performance. (3) Scalable SFT via data quantity and complexity scaling: We curate a high-quality SFT dataset collection which outperforms opensource alternatives in data quantity scaling effectiveness. We also demonstrate that training with progressively higher-complexity data surpasses baseline one-stage training by a large margin. SAIL-VL series models achieve the highest average score in 18 widely used VLM benchmarks in our evaluation, with the 2B model takes the top position over VLMs of comparable sizes on OpenCompass 2024 (this https URL) demonstrating robust visual comprehension abilities. SAIL-VL series models are released at HuggingFace.
          </p> -->

          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">SAIL-VL</h2>
      <div class="content has-text-justified">
      <p>In the existing research related to VLMs, there are two problems that lead to poor performance of lightweight VLMs:</p>
      <ul>
          <li>The quality of the basic visual understanding-type data used in VLM pre-training and the total amount of training are insufficient. This results in relatively weak basic visual understanding capabilities of the model during the pre-training stage, thus affecting the overall performance of the model after SFT.</li>
          <li>There is a lack of a unified methodology for the ratio scheme of visual instruction fine-tuning data in the VLM SFT stage and the arrangement of the training stage. Training SFT data of different scales and complexity levels together will affect the overall effect of the model.</li>
      </ul>
      <p>To address these issues, we proposed a scalable high-quality detail caption data production solution and expanded the scale of pre-training data for SAIL-VL-2B to 655B tokens, observing stable data size scaling laws. In the SFT stage, we summarized the practices of SAIL-VL in the ratio of high-quality SFT data and the design of the training stage, and also observed a trend of the model performance improving with the increase in the amount of training data.</p>

      <img src="static/images/paper_page.png" alt="pipline" width="100%">
    </div>
      </div>
    </div>
  </section> -->


  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Performance</h2>
        <div class="content has-text-justified">
          <p>The SAIL-VL series of models with parameter sizes of 2B and 8B, have achieved leading performance among
            open-source VLMs. In terms of the average scores of 18 commonly used VLM benchmarks, they outperform series
            models such as InternVL2.5-MPO and DeepSeekVL-2.</p>
        </div>
        <img src="static/images/exp.png" alt="Performance" width="100%">
      </div>
    </div>
  </section>


  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Examples</h2>
        <img src="static/images/cases.PNG" alt="cases" width="100%">

      </div>
    </div>
  </section>



  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Training Recipies</h2>
        <div class="content has-text-justified">
          <p>The SAIL-VL series of models use InterViT-300M, Qwen2.5-2B, and Qwen2.5-7B as the base models and are
            trained through two pre-training stages and three SFT stages:</p>
          <ul>
            <li>In the pre-training stage, the projector and vision encoder of SAIL-VL are gradually activated, and the
              training mainly uses SAIL-Caption and OCR data. We pointed out that using caption data with a wider
              distribution and higher-quality repeated OCR data in the pre-training stage can achieve better results.
            </li>
            <li>In the SFT stage, all the model parameters of SAIL-VL participate in the training. The data volume in
              the three SFT training stages gradually decreases, while the data complexity gradually increases.</li>
          </ul>
        </div>
        <img src="static/images/train.png" alt="training" width="100%">

      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h3 class="title">Scalable High Quality Data Construction</h3>
        <div class="content has-text-justified">
          <p>We proposed a scalable method for constructing high-quality detail caption data, achieving efficient pre-training data construction through the following four steps:</p>
          <ol>
              <li>Raw data collection: We collected a series of large-scale raw image datasets to ensure data scale and distribution diversity.</li>
              <li>Reference data production: We used the VLM API to produce a small amount of high-quality detail caption data for training a small-scale captioner model. In this step, it is necessary to ensure the distribution diversity of the sampled reference data.</li>
              <li>Recaptioner training: Use the above-mentioned reference data to train a small-scale recaptioner for large-scale data labeling.</li>
              <li>Large-scale data annotation: We implemented a multi-task, multi-node, multi-GPU parallel data annotation framework and optimized the annotation speed based on LMDeploy.</li>
          </ol>
          <p>Based on the above framework, we propose the SAIL-Caption dataset, which contains 300M high-quality Chinese and English detail caption data. Through statistical analysis, compared with open-source datasets, SAIL-Caption has a leading number of non-repeated n-grams and non-repeated POS phrases of various types, demonstrating the richness of visual elements contained in its text descriptions. In addition, we also quantitatively evaluated the data quality by comparing with various baseline datasets, and the results show that the quality of the SAIL-Caption dataset significantly outperforms other open-source datasets.</p>
        </div>
        <div class="content is-centered has-text-centered">
          <div class="is-centered">
            <img src="static/images/table2.png" alt="caption" width="55%">
          <img src="static/images/table3.png" alt="caption" width="40%">
          </div>
        </div>
        <div class="content is-centered has-text-centered">
          <div class="is-centered">
            <img src="static/images/figure3.png" alt="example" width="55%">
          <img src="static/images/figure4.png" alt="example" width="40%">
          </div>
          <p>Comparison of SAIL-Caption with other open source detail caption datasets.</p>
        </div>

      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h3 class="title">Scalable VLM Pretraining</h3>
        <div class="content has-text-justified">
          <p>We used 131B and 524B tokens for model training in the Pretrain-Alignment and Pretrain-Advance stages of SAIL-VL respectively, and verified the data size scaling laws for these two stages.</p>
          <ul>
              <li>In the Pretrain-Alignment stage, we evaluated the models trained with data volumes ranging from 2M to 64M. They found that as the training data volume increased exponentially, the performance of the models on basic visual understanding tasks showed a linear upward trend.</li>
              <img src="static/images/fig5.png" alt="example" width="100%">
              <li>In the Pretrain-Advance stage, we increased the training data volume to 256M and evaluated the models trained with different data volumes. Specifically, the pre-trained models were directly evaluated on basic understanding tasks, while the models after SFT were evaluated on 18 open-source test sets. We found that pre-trained VLMs trained with larger amounts of data not only showed a stable performance improvement on basic visual understanding tasks, but the models after SFT also exhibited a similar performance improvement trend.</li>
              <img src="static/images/fig6.png" alt="example" width="100%">
          </ul>
          <p>It is worth noting that there is a strong correlation between the performance of the pre-trained models and the models after SFT, which highlights the importance of building a VLM base with stronger basic visual understanding capabilities.</p>
          <img src="static/images/fig7.png" alt="example" width="100%">
        </div>


      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h3 class="title">Scalable VLM Fine-tuning</h3>
        <div class="content has-text-justified">
          <p>We introduced the design of SAIL-VL in the construction of high-quality SFT data and the configuration of the SFT training stage respectively, and verified the performance change trend of the SAIL-VL-2B model under different amounts of SFT training data.</p>
          <h4>High-quality SFT data construction</h4>
          <p>We proposed a set of solutions for optimizing the SFT data ratio, which is mainly divided into the following three parts:</p>
          <ol>
              <li>Quickly verify data quality: Sample a subset of the current dataset to train the model, so as to quickly verify its quality.</li>
              <li>Data component analysis: For an SFT dataset, halve the data volume of different components respectively, and gradually remove the low-quality data components.</li>
              <li>Progressive evaluation: For new SFT data, if the model performance remains unchanged or improves after adding it to the current SFT dataset, then add it to the existing SFT dataset.</li>
          </ol>
          <h4>Data Quantity Scaling</h4>
          <img src="static/images/fig8.png" alt="scaling" width="100%">
          <p>Based on the above solution, we constructed 12M-scale training data for the SAIL-Instruction stage, and compared the performance change trend of the model when using this dataset and other open-source data for training. The results show that the model trained with SAIL-Instruction achieved the best overall performance at each training data scale. In addition, we found that the performance ranking of models obtained from SFT data of different qualities at different training stages remained unchanged, which also verified the effectiveness of using a sub-dataset to train the model to verify its data quality.</p>
          <h4>Curriculum-learning-style multi-stage SFT</h4>
          <p>SAIL-VL uses a three-stage SFT training paradigm. As the training progresses, the training data has the following characteristic changes:</p>
          <ol>
              <li>The complexity of the data becomes higher, which also means that the training tasks become more challenging.</li>
              <li>The data volume is smaller, and it is more difficult to collect more complex SFT data.</li>
          </ol>
          <img src="static/images/fig9.png" alt="multi-stage" width="100%">
          <p>Compare the effects of the three-stage training scheme with the traditional all-in-one (AIO) training scheme. For the AIO baseline, we mixed the three-stage training data of SAIL-VL and combined them into one stage for training. The experimental results show that the curriculum-learning-style three-stage SFT scheme can effectively improve the overall performance of the model.</p>
        </div>


      </div>
    </div>
  </section>




  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{dong2025scalable,
        title={Scalable vision language model training via high quality data curation},
        author={Dong, Hongyuan and Kang, Zijian and Yin, Weijie and Liang, Xiao and Feng, Chao and Ran, Jiao},
        journal={arXiv preprint arXiv:2501.05952},
        year={2025}
      }</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content small">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
              
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>